{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "2YkHo48pL1M_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SdahI7VIKlb1",
        "outputId": "f0e0c70a-bf9e-4ab8-a1d6-74a78597ed3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.17.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hzieIrZLHxT",
        "outputId": "48886a86-c50e-467c-ee1c-50a7e0e305f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R5aznmO0L0dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* See the data and flatten."
      ],
      "metadata": {
        "id": "K11xM8IFL3o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, H, W = X_train.shape\n",
        "D = H * W\n",
        "X_train = X_train.reshape(-1, D)\n",
        "X_test = X_test.reshape(-1, D)\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1MCdA32LmTR",
        "outputId": "2968a7b7-b8b4-4ee4-8497-d5865fefa01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "\n",
        "X_train = X_train / 255 * 2 - 1\n",
        "X_test = X_test / 255 * 2 - 1"
      ],
      "metadata": {
        "id": "BZOZCuF_Ly0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(latent_dim):\n",
        "    i = Input(shape=(latent_dim,))\n",
        "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
        "    x = BatchNormalization(momentum=0.7)(x)\n",
        "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
        "    x = BatchNormalization(momentum=0.7)(x)\n",
        "    x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
        "    x = BatchNormalization(momentum=0.7)(x)\n",
        "    x = Dense(D, activation=\"tanh\")(x) # since we normalized layer btwn the interval -1 and 1\n",
        "\n",
        "    model = Model(i, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "4pC89hjXMKNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_size):\n",
        "    i = Input(shape=(img_size,))\n",
        "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
        "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(i, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "g_DIVQPBNGUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = build_discriminator(D)\n",
        "discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                      loss=\"binary_crossentropy\",\n",
        "                      metrics=[\"acc\"])\n",
        "\n",
        "# since we can't just train generator by itself, we should write it by an exception\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "z = Input(shape=(latent_dim,))\n",
        "generated_img = generator(z)\n",
        "discriminator.trainable = False\n",
        "fake_pred = discriminator(generated_img)\n",
        "combined_model = Model(z, fake_pred)\n",
        "combined_model.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                      loss=\"binary_crossentropy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPGybYYRN1-d",
        "outputId": "3ffa7d48-9572-4b82-ea82-0670337b1298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 30000\n",
        "sample_size = 200\n",
        "\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "if not os.path.exists(\"gan_images\"):\n",
        "    os.makedirs(\"gan_images\")"
      ],
      "metadata": {
        "id": "5IowcnyRPy3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_images(epoch):\n",
        "    rows, cols = (5, 5)\n",
        "    noise = np.random.randn(rows*cols, latent_dim)\n",
        "    imgs = generator.predict(noise)\n",
        "\n",
        "    imgs = imgs * 0.5 - 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols)\n",
        "    idx = 0\n",
        "    for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        axs[i, j].imshow(imgs[idx].reshape(H, W), cmap=\"gray\")\n",
        "        axs[i, j].axis(\"off\")\n",
        "        idx += 1\n",
        "\n",
        "    fig.savefig(\"gan_images/%d.png\" % epoch)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "92M5dfc_QaxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    N = X_train.shape[0]\n",
        "\n",
        "    # Selecting random batch number sized of images\n",
        "    idx = np.random.randint(0, N, batch_size)\n",
        "    real_imgs = X_train[idx]\n",
        "\n",
        "    # generating random batch number sized of images (32 iamge)\n",
        "    noise = np.random.randn(batch_size, latent_dim)\n",
        "    fake_imgs = generator(noise)\n",
        "\n",
        "    # discriminator losses and accs\n",
        "    d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
        "    d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
        "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "    d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
        "\n",
        "    # generator losses and accs\n",
        "    noise = np.random.randn(batch_size, latent_dim)\n",
        "    g_loss = combined_model.train_on_batch(noise, ones) # we lied by saying fake images are real (we picked ones for noies)\n",
        "\n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"epoch: {epoch}/{epochs} -------> d_loss: {d_loss} & d_acc: {d_acc} _______ g_loss:{g_loss[-1]}\")\n",
        "\n",
        "    if epoch % sample_size == 0:\n",
        "        sample_images(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFrTUKJZUssB",
        "outputId": "b1921c13-4572-4bcc-9c56-551802efc30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0/30000 -------> d_loss: 2.112969398498535 & d_acc: 0.4800196886062622 _______ g_loss:0.4797077775001526\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "epoch: 100/30000 -------> d_loss: 2.2753567695617676 & d_acc: 0.478081613779068 _______ g_loss:0.4778350591659546\n",
            "epoch: 200/30000 -------> d_loss: 2.4082608222961426 & d_acc: 0.47742098569869995 _______ g_loss:0.4772168695926666\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "epoch: 300/30000 -------> d_loss: 2.5214991569519043 & d_acc: 0.47681641578674316 _______ g_loss:0.47664234042167664\n",
            "epoch: 400/30000 -------> d_loss: 2.6183972358703613 & d_acc: 0.47616681456565857 _______ g_loss:0.4760151207447052\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "epoch: 500/30000 -------> d_loss: 2.7025656700134277 & d_acc: 0.47584068775177 _______ g_loss:0.47570621967315674\n",
            "epoch: 600/30000 -------> d_loss: 2.7786922454833984 & d_acc: 0.47578704357147217 _______ g_loss:0.47566625475883484\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "epoch: 700/30000 -------> d_loss: 2.848198413848877 & d_acc: 0.4756568670272827 _______ g_loss:0.4755472242832184\n",
            "epoch: 800/30000 -------> d_loss: 2.910506248474121 & d_acc: 0.47575968503952026 _______ g_loss:0.4756592810153961\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = X_train.shape[0]\n",
        "np.random.randint(0, N, batch_size)"
      ],
      "metadata": {
        "id": "JfqLSo4kU39N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(5.14234 * 100) // 10"
      ],
      "metadata": {
        "id": "jvVwIPegVCcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFCzRt9XaMFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}